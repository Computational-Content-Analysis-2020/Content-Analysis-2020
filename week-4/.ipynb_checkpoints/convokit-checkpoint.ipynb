{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Analysis Toolkit\n",
    "\n",
    "This notebook will build on the work in the orienting Networks notebook: we will primarily be using CornellNLP's [Conversational Analysis Toolkit](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit). From their page:\n",
    "\n",
    "*This toolkit contains tools to extract conversational features and analyze social phenomena in conversations, using a single unified interface inspired by (and compatible with) scikit-learn. Several large conversational datasets are included together with scripts exemplifying the use of the toolkit on these datasets.*\n",
    "\n",
    "We are going to demonstrate some of these, using the examples which they have on their GitHub page, and also encourage you all to check them out yourself!\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucem_illud_2020\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneGroupOut\n",
    "from sklearn.feature_selection import f_classif, SelectPercentile\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import convokit\n",
    "from convokit import download\n",
    "from convokit.prompt_types import PromptTypeWrapper\n",
    "from convokit import PolitenessStrategies\n",
    "from convokit import Corpus\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first experiment will be in finding trends of coordination in conversations. We will use their built in Supreme Court dataset to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = convokit.Corpus(filename=convokit.download(\"supreme-corpus\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordination measures\n",
    "\n",
    "The following section explores some of the metrics explored in the [Echoes of Power](https://www.cs.cornell.edu/~cristian/Echoes_of_power.html) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the coordination metrics on this corpus and print the ranked coordination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute coordination scores on this corpus\n",
    "coord = convokit.Coordination()\n",
    "coord.fit(corpus)\n",
    "\n",
    "# get coordination scores\n",
    "coord.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get set of all justices\n",
    "justices = list(corpus.iter_users(lambda user: user.meta[\"is-justice\"]))\n",
    "# get set of all users\n",
    "everyone = list(corpus.iter_users())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute coordination from each justice to everyone\n",
    "print(\"Justices, ranked by how much they coordinate to others:\")\n",
    "justices_to_everyone = coord.score(corpus, justices, everyone)\n",
    "for justice, score in sorted(justices_to_everyone.averages_by_user().items(),\n",
    "    key=lambda x: x[1], reverse=True):\n",
    "    print(justice.id, round(score, 5))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute coordination from everyone to each justice\n",
    "print(\"Justices, ranked by how much others coordinate to them:\")\n",
    "everyone_to_justices = coord.score(corpus, everyone, justices, focus=\"targets\")\n",
    "for justice, score in sorted(everyone_to_justices.averages_by_user().items(), \n",
    "    key=lambda x: x[1], reverse=True):\n",
    "    print(justice.id, round(score, 5))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [GitHub examples page](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/coordination/examples.ipynb) for Coordination provide further results from the Echoes of Power paper.\n",
    "\n",
    "## Politeness Strategies\n",
    "\n",
    "The following section demonstrates how to explore politness strategies and prompt types, as described in [ Conversations Gone Awry: Detecting Early Signs of Conversational Failure](http://www.cs.cornell.edu/~cristian/Conversations_gone_awry.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  OPTION 1: DOWNLOAD CORPUS \n",
    "# UNCOMMENT THESE LINES TO DOWNLOAD CORPUS\n",
    "DATA_DIR = '../data'\n",
    "AWRY_ROOT_DIR = download('conversations-gone-awry-corpus', data_dir=DATA_DIR)\n",
    "\n",
    "# OPTION 2: READ PREVIOUSLY-DOWNLOADED CORPUS FROM DISK\n",
    "# UNCOMMENT THIS LINE AND REPLACE WITH THE DIRECTORY WHERE THE TENNIS-CORPUS IS LOCATED\n",
    "# AWRY_ROOT_DIR = '<YOUR DIRECTORY>'\n",
    "\n",
    "awry_corpus = Corpus(AWRY_ROOT_DIR)\n",
    "awry_corpus.load_info('utterance', ['parsed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a larger dataset, so we only use 2018 occurences now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, construct a table of conversations that meet the filter criteria (annotation_year = '2018')\n",
    "kept_conversations = {c.id: c for c in awry_corpus.iter_conversations() if c.meta['annotation_year'] == \"2018\"}\n",
    "# next, construct a filtered utterance table containing only the utterances in the filtered conversations\n",
    "kept_utterances = {}\n",
    "for convo_id in kept_conversations:\n",
    "    for utterance in kept_conversations[convo_id].iter_utterances():\n",
    "        kept_utterances[utterance.id] = utterance\n",
    "# finally, we overwrite the `conversations` and `utterances` fields of the Corpus object to turn it into a filtered Corpus.\n",
    "awry_corpus.conversations = kept_conversations\n",
    "awry_corpus.utterances = kept_utterances\n",
    "# make sure the size is what we expect\n",
    "print(len(awry_corpus.conversations))\n",
    "print(len(awry_corpus.utterances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting prompt features\n",
    "\n",
    "In this step, we will extract the first of the two types of pragmatic features seen in the paper: prompt types. We can learn prompt types and compute types for each utterance in the corpus using ConvoKit's PromptTypeWrapper class (which implements an end-to-end pipeline that extracts prompt types; see [this notebook](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/prompt-types/prompt-type-demo.ipynb) and [this notebook](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/prompt-types/prompt-type-wrapper-demo.ipynb) for examples of particular steps in that pipeline). Note that in keeping with proper machine learning practices, we need a different dataset to use for training the PromptTypeWrapper object. For this, we will use Convokit's Wikipedia talk corpus (\"wiki-corpus\"), which is comprised of Wikipedia talk page conversations different from those found in the Awry corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: DOWNLOAD CORPUS \n",
    "# UNCOMMENT THESE LINES TO DOWNLOAD CORPUS\n",
    "FULL_ROOT_DIR = download('wiki-corpus', data_dir=DATA_DIR)\n",
    "\n",
    "# OPTION 2: READ PREVIOUSLY-DOWNLOADED CORPUS FROM DISK\n",
    "# UNCOMMENT THIS LINE AND REPLACE WITH THE DIRECTORY WHERE THE TENNIS-CORPUS IS LOCATED\n",
    "# FULL_ROOT_DIR = '<YOUR DIRECTORY>'\n",
    "\n",
    "full_corpus = Corpus(FULL_ROOT_DIR)\n",
    "full_corpus.load_info('utterance', ['parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = PromptTypeWrapper(n_types=6, use_prompt_motifs=False, root_only=False,\n",
    "                            questions_only=False, enforce_caps=False, min_support=20, min_df=100,\n",
    "                            svd__n_components=50, max_dist=1., random_state=1000)\n",
    "pt_model.fit(full_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the six prompt types that our model has inferred -- i.e., prototypical features of the prompts and responses, as well as prototypical utterances for each type.\n",
    "Note that due to updates in SpaCy's dependency parsing, as well as in our implementation of the prompt types methodology, the particular types returned don't fully align with what's in the paper (though we see that they recover similar prompt types, and down the line may result in better performance accuracies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    print(i)\n",
    "    pt_model.display_type(i,k=25, corpus=full_corpus)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've given them the following names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_NAMES = ['Prompt: Casual', 'Prompt: Moderation', 'Prompt: Coordination', 'Prompt: Contention',\n",
    "             'Prompt: Editing', 'Prompt: Procedures']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awry_corpus = pt_model.transform(awry_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to gather the computed prompt types into a tabular format to use as features for an sklearn estimator. For our purposes, we want the distances from the centers of the KMeans clusters corresponding to each prompt type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dist_df = pd.DataFrame(index=awry_corpus.vector_reprs['prompt_types__prompt_dists.6']['keys'],\n",
    "                         data=awry_corpus.vector_reprs['prompt_types__prompt_dists.6']['vects'])\n",
    "type_ids = np.argmin(prompt_dist_df.values, axis=1)\n",
    "mask  = np.min(prompt_dist_df.values, axis=1) >= 1.\n",
    "type_ids[mask] = 6\n",
    "\n",
    "prompt_dist_df.columns = ['km_%d_dist' % c for c in prompt_dist_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dist_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_type_assignments = np.zeros((len(prompt_dist_df), prompt_dist_df.shape[1]+1))\n",
    "prompt_type_assignments[np.arange(len(type_ids)),type_ids] = 1\n",
    "prompt_type_assignment_df = pd.DataFrame(columns=np.arange(prompt_dist_df.shape[1]+1), index=prompt_dist_df.index, \n",
    "                                        data=prompt_type_assignments)\n",
    "prompt_type_assignment_df = prompt_type_assignment_df[prompt_type_assignment_df.columns[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_type_assignment_df.columns = TYPE_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_type_assignment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get vector representations of the prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awry_corpus.get_vect_repr(\"143902946.11991.11991\", 'prompt_types__prompt_repr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other notebooks linked earlier on have more work on prompts; these models were described here: [Asking Too Much? The Rhetorical Role of Questions in Political Discourse](http://www.cs.cornell.edu/~cristian/Asking_too_much.html)\n",
    "\n",
    "We now move on to the politeness analysis.\n",
    "\n",
    "### Extracting politeness features\n",
    "\n",
    "Now we will extract the second type of pragmatic features described in the paper: politeness strategies. We can do this using convokit's PolitenessStrategies class. This class does not require any training, so we can just apply it directly to the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PolitenessStrategies(verbose=1000)\n",
    "awry_corpus = ps.transform(awry_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_ids = awry_corpus.get_utterance_ids()\n",
    "rows = []\n",
    "for uid in utterance_ids:\n",
    "    rows.append(awry_corpus.get_utterance(uid).meta[\"politeness_strategies\"])\n",
    "politeness_strategies = pd.DataFrame(rows, index=utterance_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politeness_strategies.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create paired data: the analyses performed in the paper, and the prediction task, considers pairs of conversations. The corpus downloaded from convokit already includes metadata about how conversations were paired for the paper, so we don't need to do any of the hard work here. Instead, we'll format the pair information into a table for use in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to directly map comment IDs to their conversations. We'll build a DataFrame to do this\n",
    "comment_ids = []\n",
    "convo_ids = []\n",
    "timestamps = []\n",
    "page_ids = []\n",
    "for conversation in awry_corpus.iter_conversations():\n",
    "    for comment in conversation.iter_utterances():\n",
    "        # section headers are included in the dataset for completeness, but for prediction we need to ignore\n",
    "        # them as they are not utterances\n",
    "        if not comment.meta[\"is_section_header\"]:\n",
    "            comment_ids.append(comment.id)\n",
    "            convo_ids.append(comment.root)\n",
    "            timestamps.append(comment.timestamp)\n",
    "            page_ids.append(conversation.meta[\"page_id\"])\n",
    "comment_df = pd.DataFrame({\"conversation_id\": convo_ids, \"timestamp\": timestamps, \"page_id\": page_ids}, index=comment_ids)\n",
    "\n",
    "# we'll do our construction using awry conversation ID's as the reference key\n",
    "awry_convo_ids = set()\n",
    "# these dicts will then all be keyed by awry ID\n",
    "good_convo_map = {}\n",
    "page_id_map = {}\n",
    "for conversation in awry_corpus.iter_conversations():\n",
    "    if conversation.meta[\"conversation_has_personal_attack\"] and conversation.id not in awry_convo_ids:\n",
    "        awry_convo_ids.add(conversation.id)\n",
    "        good_convo_map[conversation.id] = conversation.meta[\"pair_id\"]\n",
    "        page_id_map[conversation.id] = conversation.meta[\"page_id\"]\n",
    "awry_convo_ids = list(awry_convo_ids)\n",
    "pairs_df = pd.DataFrame({\"bad_conversation_id\": awry_convo_ids,\n",
    "                         \"conversation_id\": [good_convo_map[cid] for cid in awry_convo_ids],\n",
    "                         \"page_id\": [page_id_map[cid] for cid in awry_convo_ids]})\n",
    "# finally, we will augment the pairs dataframe with the IDs of the first and second comment for both\n",
    "# the bad and good conversation. This will come in handy for constructing the feature matrix.\n",
    "first_ids = []\n",
    "second_ids = []\n",
    "first_ids_bad = []\n",
    "second_ids_bad = []\n",
    "for row in pairs_df.itertuples():\n",
    "    # \"first two\" is defined in terms of time of posting\n",
    "    comments_sorted = comment_df[comment_df.conversation_id==row.conversation_id].sort_values(by=\"timestamp\")\n",
    "    first_ids.append(comments_sorted.iloc[0].name)\n",
    "    second_ids.append(comments_sorted.iloc[1].name)\n",
    "    comments_sorted_bad = comment_df[comment_df.conversation_id==row.bad_conversation_id].sort_values(by=\"timestamp\")\n",
    "    first_ids_bad.append(comments_sorted_bad.iloc[0].name)\n",
    "    second_ids_bad.append(comments_sorted_bad.iloc[1].name)\n",
    "pairs_df = pairs_df.assign(first_id=first_ids, second_id=second_ids, \n",
    "                           bad_first_id=first_ids_bad, bad_second_id=second_ids_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare how often the pragmatic devices we extracted occur in the initial exchanges of conversations that turn awry, vs. conversations that stay on track. We will compute log-odds ratios of each device, comparing the awry and on-track conversations; we will also compute significance values from binomal tests comparing the proportion of awry-turning conversations exhibiting a particular device to the proportion of on-track conversations.\n",
    "Since we've already got the pragmatic features precomputed, and our dataset of pairs compiled, it remains to compute the effect sizes and statistical significances, and plot these values, producing a plot like Figure 2 from the paper.\n",
    "Note: due to changes in SpaCy's dependency parsing that took place between the original time of publication and the updated release of this code, the extracted features may differ slightly from the ones used in the paper, so the resulting figure may differ slightly from the one in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_feature_name(feat):\n",
    "    new_feat = feat.replace('feature_politeness','').replace('==','').replace('_', ' ')\n",
    "    split = new_feat.split()\n",
    "    first, rest = split[0], ' '.join(split[1:]).lower()\n",
    "    if first[0].isalpha():\n",
    "        first = first.title()\n",
    "    if 'Hashedge' in first:\n",
    "        return 'Hedge (lexicon)'\n",
    "    if 'Hedges' in first:\n",
    "        return 'Hedge (dep. tree)'\n",
    "    if 'greeting' in feat:\n",
    "        return 'Greetings'\n",
    "    cleaner_str = first + ' ' + rest\n",
    "#     cleaner_str = cleaner_str.replace('2nd', '2$\\mathregular{^{nd}}$').replace('1st', '1$\\mathregular{^{st}}$')\n",
    "    return cleaner_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politeness_strategies_display = politeness_strategies[[col for col in politeness_strategies.columns \n",
    "                  if col not in ['feature_politeness_==HASNEGATIVE==', 'feature_politeness_==HASPOSITIVE==']]].copy()\n",
    "politeness_strategies_display.columns = [clean_feature_name(col) for col in politeness_strategies_display.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = politeness_strategies_display.join(prompt_type_assignment_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tox_first_comment_features =pairs_df[['bad_first_id']].join(all_features, how='left', on='bad_first_id')[all_features.columns]\n",
    "ntox_first_comment_features =pairs_df[['first_id']].join(all_features, how='left', on='first_id')[all_features.columns]\n",
    "\n",
    "tox_second_comment_features =pairs_df[['bad_second_id']].join(all_features, how='left', on='bad_second_id')[all_features.columns]\n",
    "ntox_second_comment_features =pairs_df[['second_id']].join(all_features, how='left', on='second_id')[all_features.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_stars(x):\n",
    "    if x < .001: return '***'\n",
    "    elif x < .01: return '**'\n",
    "    elif x < .05: return '*'\n",
    "    else: return ''\n",
    "def compare_tox(df_ntox, df_tox,  min_n=0):\n",
    "    cols = df_ntox.columns\n",
    "    num_feats_in_tox = df_tox[cols].sum().astype(int).rename('num_feat_tox')\n",
    "    num_nfeats_in_tox = (1 - df_tox[cols]).sum().astype(int).rename('num_nfeat_tox')\n",
    "    num_feats_in_ntox = df_ntox[cols].sum().astype(int).rename('num_feat_ntox')\n",
    "    num_nfeats_in_ntox = (1 - df_ntox[cols]).sum().astype(int).rename('num_nfeat_ntox')\n",
    "    prop_tox = df_tox[cols].mean().rename('prop_tox')\n",
    "    ref_prop_ntox = df_ntox[cols].mean().rename('prop_ntox')\n",
    "    n_tox = len(df_tox)\n",
    "    df = pd.concat([\n",
    "        num_feats_in_tox, \n",
    "        num_nfeats_in_tox,\n",
    "        num_feats_in_ntox,\n",
    "        num_nfeats_in_ntox,\n",
    "        prop_tox,\n",
    "        ref_prop_ntox,\n",
    "    ], axis=1)\n",
    "    df['num_total'] = df.num_feat_tox + df.num_feat_ntox\n",
    "    df['log_odds'] = np.log(df.num_feat_tox) - np.log(df.num_nfeat_tox) \\\n",
    "        + np.log(df.num_nfeat_ntox) - np.log(df.num_feat_ntox)\n",
    "    df['abs_log_odds'] = np.abs(df.log_odds)\n",
    "    df['binom_p'] = df.apply(lambda x: stats.binom_test(x.num_feat_tox, n_tox, x.prop_ntox), axis=1)\n",
    "    df = df[df.num_total >= min_n]\n",
    "    df['p'] = df['binom_p'].apply(lambda x: '%.3f' % x)\n",
    "    df['pstars'] = df['binom_p'].apply(get_p_stars)\n",
    "    return df.sort_values('log_odds', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_comparisons = compare_tox(ntox_first_comment_features, tox_first_comment_features)\n",
    "second_comparisons = compare_tox(ntox_second_comment_features, tox_second_comment_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now ready to plot these comparisons. the following (rather intimidating) helper function \n",
    "# produces a nicely-formatted plot:\n",
    "def draw_figure(ax, first_cmp, second_cmp, title='', prompt_types=6, min_log_odds=.2, min_freq=50,xlim=.85):\n",
    "\n",
    "    # selecting and sorting the features to plot, given minimum effect sizes and statistical significance\n",
    "    frequent_feats = first_cmp[first_cmp.num_total >= min_freq].index.union(second_cmp[second_cmp.num_total >= min_freq].index)\n",
    "    lrg_effect_feats = first_cmp[(first_cmp.abs_log_odds >= .2)\n",
    "                                & (first_cmp.binom_p < .05)].index.union(second_cmp[(second_cmp.abs_log_odds >= .2)\n",
    "                                                                                  & (second_cmp.binom_p < .05)].index)\n",
    "    feats_to_include = frequent_feats.intersection(lrg_effect_feats)\n",
    "    feat_order = sorted(feats_to_include, key=lambda x: first_cmp.loc[x].log_odds, reverse=True)\n",
    "\n",
    "    # parameters determining the look of the figure\n",
    "    colors = ['darkorchid', 'seagreen']\n",
    "    shapes = ['d', 's']    \n",
    "    eps = .02\n",
    "    star_eps = .035\n",
    "    xlim = xlim\n",
    "    min_log = .2\n",
    "    gap_prop = 2\n",
    "    label_size = 14\n",
    "    title_size=18\n",
    "    radius = 144\n",
    "    features = feat_order\n",
    "    ax.invert_yaxis()\n",
    "    ax.plot([0,0], [0, len(features)/gap_prop], color='black')\n",
    "    \n",
    "    # for each figure we plot the point according to effect size in the first and second comment, \n",
    "    # and add axis labels denoting statistical significance\n",
    "    yticks = []\n",
    "    yticklabels = []\n",
    "    for f_idx, feat in enumerate(features):\n",
    "        curr_y = (f_idx + .5)/gap_prop\n",
    "        yticks.append(curr_y)\n",
    "        try:\n",
    "            \n",
    "            first_p = first_cmp.loc[feat].binom_p\n",
    "            second_p = second_cmp.loc[feat].binom_p            \n",
    "            if first_cmp.loc[feat].abs_log_odds < min_log:\n",
    "                first_face = \"white\"\n",
    "            elif first_p >= 0.05:\n",
    "                first_face = 'white'\n",
    "            else:\n",
    "                first_face = colors[0]\n",
    "            if second_cmp.loc[feat].abs_log_odds < min_log:\n",
    "                second_face = \"white\"\n",
    "            elif second_p >= 0.05:\n",
    "                second_face = 'white'\n",
    "            else:\n",
    "                second_face = colors[1]\n",
    "            ax.plot([-1 * xlim, xlim], [curr_y, curr_y], '--', color='grey', zorder=0, linewidth=.5)\n",
    "            \n",
    "            ax.scatter([first_cmp.loc[feat].log_odds], [curr_y + eps], s=radius, edgecolor=colors[0], marker=shapes[0],\n",
    "                        zorder=20, facecolors=first_face)\n",
    "            ax.scatter([second_cmp.loc[feat].log_odds], [curr_y + eps], s=radius, edgecolor=colors[1], marker=shapes[1], \n",
    "                       zorder=10, facecolors=second_face)\n",
    "            \n",
    "            first_pstr_len = len(get_p_stars(first_p))\n",
    "            second_pstr_len = len(get_p_stars(second_p))\n",
    "            p_str = np.array([' '] * 8)\n",
    "            if first_pstr_len > 0:\n",
    "                p_str[:first_pstr_len] = '*'\n",
    "            if second_pstr_len > 0:\n",
    "                p_str[-second_pstr_len:] = 'âº'\n",
    "            \n",
    "            feat_str = feat + '\\n' + ''.join(p_str)\n",
    "            yticklabels.append(feat_str)\n",
    "        except Exception as e:\n",
    "            yticklabels.append('')\n",
    "    \n",
    "    # add the axis labels\n",
    "    ax.set_xlabel('log-odds ratio', fontsize=14, family='serif')\n",
    "    ax.set_xticks([-xlim-.05, -.5, 0, .5, xlim])\n",
    "    ax.set_xticklabels(['on-track', -.5, 0, .5, 'awry'], fontsize=14, family='serif')\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_yticklabels(yticklabels, fontsize=16, family='serif')\n",
    "    ax.tick_params(axis='both',  which='both', bottom='off',  top='off',left='off')\n",
    "    if title != '':\n",
    "        ax.text(0, (len(features) + 2.25)/ gap_prop, title, fontsize=title_size, family='serif',horizontalalignment='center',)\n",
    "    return feat_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,1, figsize=(5,10))\n",
    "_ = draw_figure(ax, first_comparisons, second_comparisons, 'First & second comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing Feature Matrix\n",
    "\n",
    "To run the prediction task, we can construct a table of pragmatic features for each pair, to use in prediction. This table will consist of the prompt types and politeness strategies for the first and second comment of each conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_for_convo(convo_id, first_comment_id, second_comment_id):\n",
    "\n",
    "    # get prompt type features\n",
    "    try:\n",
    "        first_prompts = prompt_dist_df.loc[first_comment_id]\n",
    "    except:\n",
    "        first_prompts = pd.Series(data=np.ones(len(prompt_dist_df.columns)), index=prompt_dist_df.columns)\n",
    "    try:\n",
    "        second_prompts = prompt_dist_df.loc[second_comment_id].rename({c: c + \"_second\" for c in prompt_dist_df.columns})\n",
    "    except:\n",
    "        second_prompts = pd.Series(data=np.ones(len(prompt_dist_df.columns)), index=[c + \"_second\" for c in prompt_dist_df.columns])\n",
    "    prompts = first_prompts.append(second_prompts)\n",
    "    # get politeness strategies features\n",
    "    first_politeness = politeness_strategies.loc[first_comment_id]\n",
    "    second_politeness = politeness_strategies.loc[second_comment_id].rename({c: c + \"_second\" for c in politeness_strategies.columns})\n",
    "    politeness = first_politeness.append(second_politeness)\n",
    "    return politeness.append(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_ids = np.concatenate((pairs_df.conversation_id.values, pairs_df.bad_conversation_id.values))\n",
    "feats = [features_for_convo(row.conversation_id, row.first_id, row.second_id) for row in pairs_df.itertuples()] + \\\n",
    "        [features_for_convo(row.bad_conversation_id, row.bad_first_id, row.bad_second_id) for row in pairs_df.itertuples()]\n",
    "feature_table = pd.DataFrame(data=np.vstack([f.values for f in feats]), columns=feats[0].index, index=convo_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the paper, we dropped the sentiment lexicon based features (HASPOSITIVE and HASNEGATIVE), opting\n",
    "# to instead use them as a baseline. We do this here as well to be consistent with the paper.\n",
    "feature_table = feature_table.drop(columns=[\"feature_politeness_==HASPOSITIVE==\",\n",
    "                                            \"feature_politeness_==HASNEGATIVE==\",\n",
    "                                            \"feature_politeness_==HASPOSITIVE==_second\",\n",
    "                                            \"feature_politeness_==HASNEGATIVE==_second\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up some prediction helper functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(seq):\n",
    "    vals, counts = np.unique(seq, return_counts=True)\n",
    "    return vals[np.argmax(counts)]\n",
    "\n",
    "def run_pred_single(inputs, X, y):\n",
    "    f_idx, (train_idx, test_idx) = inputs\n",
    "    \n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    base_clf = Pipeline([(\"scaler\", StandardScaler()), (\"featselect\", SelectPercentile(f_classif, 10)), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "    clf = GridSearchCV(base_clf, {\"logreg__C\": [10**i for i in range(-4,4)], \"featselect__percentile\": list(range(10, 110, 10))}, cv=3)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_scores = clf.predict_proba(X_test)[:,1]\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    feature_weights = clf.best_estimator_.named_steps[\"logreg\"].coef_.flatten()\n",
    "    feature_mask = clf.best_estimator_.named_steps[\"featselect\"].get_support()\n",
    "    \n",
    "    hyperparams = clf.best_params_\n",
    "    \n",
    "    return (y_pred, y_scores, feature_weights, hyperparams, feature_mask)\n",
    "\n",
    "def run_pred(X, y, fnames, groups):\n",
    "    feature_weights = {}\n",
    "    scores = np.asarray([np.nan for i in range(len(y))])\n",
    "    y_pred = np.zeros(len(y))\n",
    "    hyperparameters = defaultdict(list)\n",
    "    splits = list(enumerate(LeaveOneGroupOut().split(X, y, groups)))\n",
    "    accs = []\n",
    "        \n",
    "    with Pool(os.cpu_count()) as p:\n",
    "        prediction_results = p.map(partial(run_pred_single, X=X, y=y), splits)\n",
    "        \n",
    "    fselect_pvals_all = []\n",
    "    for i in range(len(splits)):\n",
    "        f_idx, (train_idx, test_idx) = splits[i]\n",
    "        y_pred_i, y_scores_i, weights_i, hyperparams_i, mask_i = prediction_results[i]\n",
    "        y_pred[test_idx] = y_pred_i\n",
    "        scores[test_idx] = y_scores_i\n",
    "        feature_weights[f_idx] = np.asarray([np.nan for _ in range(len(fnames))])\n",
    "        feature_weights[f_idx][mask_i] = weights_i\n",
    "        for param in hyperparams_i:\n",
    "            hyperparameters[param].append(hyperparams_i[param])   \n",
    "    \n",
    "    acc = np.mean(y_pred == y)\n",
    "    pvalue = stats.binom_test(sum(y_pred == y), n=len(y), alternative=\"greater\")\n",
    "                \n",
    "    coef_df = pd.DataFrame(feature_weights, index=fnames)\n",
    "    coef_df['mean_coef'] = coef_df.apply(np.nanmean, axis=1)\n",
    "    coef_df['std_coef'] = coef_df.apply(np.nanstd, axis=1)\n",
    "    return acc, coef_df[['mean_coef', 'std_coef']], scores, pd.DataFrame(hyperparameters), pvalue\n",
    "\n",
    "def get_labeled_pairs(pairs_df):\n",
    "    paired_labels = []\n",
    "    c0s = []\n",
    "    c1s = []\n",
    "    page_ids = []\n",
    "    for i, row in enumerate(pairs_df.itertuples()):\n",
    "        if i % 2 == 0:\n",
    "            c0s.append(row.conversation_id)\n",
    "            c1s.append(row.bad_conversation_id)\n",
    "        else:\n",
    "            c0s.append(row.bad_conversation_id)\n",
    "            c1s.append(row.conversation_id)\n",
    "        paired_labels.append(i%2)\n",
    "        page_ids.append(row.page_id)\n",
    "    return pd.DataFrame({\"c0\": c0s, \"c1\": c1s,\"first_convo_toxic\": paired_labels, \"page_id\": page_ids})\n",
    "\n",
    "def get_feature_subset(labeled_pairs_df, feature_list):\n",
    "    prompt_type_names = [\"km_%d_dist\" % i for i in range(6)] + [\"km_%d_dist_second\" % i for i in range(6)]\n",
    "    politeness_names = [f for f in feature_table.columns if f not in prompt_type_names]\n",
    "    \n",
    "    features_to_use = []\n",
    "    if \"prompt_types\" in feature_list:\n",
    "        features_to_use += prompt_type_names\n",
    "    if \"politeness_strategies\" in feature_list:\n",
    "        features_to_use += politeness_names\n",
    "        \n",
    "    feature_subset = feature_table[features_to_use]\n",
    "    \n",
    "    c0_feats = feature_subset.loc[labeled_pairs_df.c0].values\n",
    "    c1_feats = feature_subset.loc[labeled_pairs_df.c1].values\n",
    "    \n",
    "    return c0_feats, c1_feats, features_to_use\n",
    "\n",
    "def run_pipeline(feature_set):\n",
    "    print(\"Running prediction task for feature set\", \"+\".join(feature_set))\n",
    "    print(\"Generating labels...\")\n",
    "    labeled_pairs_df = get_labeled_pairs(pairs_df)\n",
    "    print(\"Computing paired features...\")\n",
    "    X_c0, X_c1, feature_names = get_feature_subset(labeled_pairs_df, feature_set)\n",
    "    X = X_c1 - X_c0\n",
    "    print(\"Using\", X.shape[1], \"features\")\n",
    "    y = labeled_pairs_df.first_convo_toxic.values\n",
    "    print(\"Running leave-one-page-out prediction...\")\n",
    "    accuracy, coefs, scores, hyperparams, pvalue = run_pred(X, y, feature_names, labeled_pairs_df.page_id)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"p-value: %.4e\" % pvalue)\n",
    "    print(\"C (mode):\", mode(hyperparams.logreg__C))\n",
    "    print(\"Percent of features (mode):\", mode(hyperparams.featselect__percentile))\n",
    "    print(\"Coefficents:\")\n",
    "    print(coefs.sort_values(by=\"mean_coef\"))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_combos = [[\"politeness_strategies\"], [\"prompt_types\"], [\"politeness_strategies\", \"prompt_types\"]]\n",
    "combo_names = []\n",
    "accs = []\n",
    "for combo in feature_combos:\n",
    "    combo_names.append(\"+\".join(combo).replace(\"_\", \" \"))\n",
    "    accuracy = run_pipeline(combo)\n",
    "    accs.append(accuracy)\n",
    "results_df = pd.DataFrame({\"Accuracy\": accs}, index=combo_names)\n",
    "results_df.index.name = \"Feature set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've more or less recreated the results from the paper!\n",
    "\n",
    "## Hypergraph Conversation Representation\n",
    "\n",
    "The following are examples of how to call the hyperconvo functions from Convokit and perform analyses similar to the ones presented in the corresponding paper [Characterizing Online Public Discussions through Patterns of Participant Interactions ](http://www.cs.cornell.edu/~cristian/Patterns_of_participant_interactions.html), which describes the hypergraph methodology for modeling and analyzing online public discussions.\n",
    "\n",
    "We use reddit threads to model the conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = convokit.Corpus(filename=convokit.download(\"reddit-corpus-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.print_summary_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_utterance_ids = [utt.id for utt in corpus.iter_utterances() if utt.id == utt.meta['top_level_comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_level_utterance_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(corpus.iter_conversations()).meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_corpus = corpus.reindex_conversations(new_convo_roots=top_level_utterance_ids, \n",
    "                                              preserve_convo_meta=True,\n",
    "                                              preserve_corpus_meta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.get_utterance('9bzh9g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.print_summary_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_corpus.print_summary_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the structure of the thread: the reply-structure (where subsequent replies in the comment tree are indented), and the authors of each comment. There are some high-level qualitative differences that you might immediately see; our goal is to automatically extract a superset of these intuitive distinctions. For instance, some threads have a very linear structure, while others are flatter; to capture this difference, we might compute statistics on the distribution of in-degrees of nodes in the reply tree. We also intuit that some conversations involve a few people replying repeatedly to each other, whereas others may involve users stopping by to chime in once and then leaving; we'll later codify this difference via statistics on the indegrees and outdegrees of hypernodes (users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_threads = ['e57u6ft', 'e56rtrx']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_corpus.get_conversation('e57u6ft').print_conversation_structure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_corpus.get_conversation('e56rtrx').print_conversation_structure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_corpus.get_conversation('e56rtrx').print_conversation_structure(lambda utt: utt.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One technical note: Since we don't have access to the author of deleted comments, we make the assumption for now that each deleted comment in a particular thread was written by the same author -- a potential source of noise in the subsequent analyses.\n",
    "Typically, to extract hypergraph features, we would simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hyperconvo object and use it to extract features\n",
    "hc = convokit.HyperConvo(prefix_len=10, min_thread_len=10)\n",
    "hc.fit_transform(threads_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo1 = next(threads_corpus.iter_conversations())\n",
    "print(convo1.id)\n",
    "convo1.meta['hyperconvo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperconvo's fit_transform() method generates features data for each thread (either starting with the post or top-level comment) and stores it in the corresponding Conversation's (which is indexed by post id) metadata. We collect the thread features from each Conversation to do some aggregated analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_feats = dict()\n",
    "\n",
    "for convo in threads_corpus.iter_conversations():\n",
    "    threads_feats[convo.id] = convo.meta['hyperconvo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_value(x):\n",
    "    if np.isinf(x) or np.isnan(x):\n",
    "        return -1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = list(next(iter(threads_feats.values())).keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_ids = []\n",
    "feats = []\n",
    "for key, feat_dict in threads_feats.items():\n",
    "    thread_ids.append(key)\n",
    "    feats.append([clean_value(feat_dict[k]) for k in feat_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later convenience we will store feature values in a dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = pd.DataFrame(data=feats, index=thread_ids, columns=feat_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of features computed over the three example threads from before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_C_indegree_feats = [x for x in feat_names if 'indegree over C->C responses' in x]\n",
    "C_C_mid_outdegree_feats = [x for x in feat_names if 'outdegree over C->C mid-thread responses' in x]\n",
    "motif_count_feats = [x for x in feat_names if ('count' in x) and ('mid' not in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features derived from the distribution of hypernode to hypernode indegrees (i.e., \"how many other people are responding to a particular discussion participant?\"). As in the paper, we compute various summary statistics over the distribution.\n",
    "As noted at the start of this notebook, since we do not have access to reaction information, these distributions encompass only the reply structure within the thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.loc[demo_threads][C_C_indegree_feats].T.sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features derived from the distribution of hypernode to hypernode outdegrees in the middle of the thread (i.e., \"beyond the root comment, how many other people is each participant responding to?\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.loc[demo_threads][C_C_mid_outdegree_feats].T.sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features derived from the 5 motifs considered in the paper. Note that because we do not have reply information, we instead take as features counts of each motif (in the paper, we would additionally compare reply or reaction edge types within motifs of a particular form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.loc[demo_threads][motif_count_feats].T.sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the features generated above can be used as is (e.g., as features in a prediction task), we can also interpret them by projecting them into a low-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mtx = SimpleImputer(missing_values=-1).fit_transform(feat_df.values)\n",
    "feat_mtx = StandardScaler().fit_transform(feat_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=7, algorithm='arpack') # deals with an issue where the randomized alg hangs\n",
    "svd.fit(feat_mtx)\n",
    "U, s, V = svd.transform(feat_mtx) / svd.singular_values_, \\\n",
    "        svd.singular_values_, \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_norm = Normalizer().fit_transform(U)\n",
    "V_norm = Normalizer().fit_transform(V)\n",
    "U_df = pd.DataFrame(data=U_norm, index=feat_df.index)\n",
    "V_df = pd.DataFrame(data=V_norm, index=feat_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This embedding procedure produces embeddings of threads in the low-dimensional space:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_df.loc[demo_threads].T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as embeddings of features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_df.loc[C_C_indegree_feats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo1.meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the paper, for further interpretability we can consider embeddings of communities (subreddits, standing for Facebook pages) in terms of the discussions they foster, by averaging the embeddings of all threads in a particular subreddit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [convo.meta['original_convo_meta']['subreddit'] for convo in threads_corpus.iter_conversations()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subreddits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_df['subreddit'] = subreddits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_means = U_df.groupby('subreddit').mean()\n",
    "subreddit_df = pd.DataFrame(\n",
    "        data=Normalizer().fit_transform(subreddit_means.values),\n",
    "        index = subreddit_means.index\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's give a rough overview of the space we've sketched out through this procedure, by visualizing the subreddit embeddings using the TSNE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(random_state=2018)\n",
    "tsne_df = pd.DataFrame(data=tsne.fit_transform(subreddit_df.values),\n",
    "                      index=subreddit_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(tsne_df[0].values, tsne_df[1].values)\n",
    "for i, txt in enumerate(tsne_df.index):\n",
    "    plt.annotate(txt, (tsne_df.values[i,0], tsne_df.values[i,1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking across this plot, we can spot a few interpretable-looking groupings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = [\"battlefield3\", \"Diablo\", \"DotA2\", \"Guildwars2\", \"leagueoflegends\", \"magicTCG\", \"Minecraft\", \"pokemon\", \"skyrim\", \"starcraft\", \"tf2\", \"wow\"]\n",
    "trade = [\"Dota2Trade\", \"pokemontrades\", \"SteamGameSwap\", \"tf2trade\", \"Random_Acts_Of_Amazon\"]\n",
    "sports = [\"baseball\", \"CFB\", \"hockey\", \"MMA\", \"nba\", \"nfl\", \"soccer\"]\n",
    "\n",
    "link_aggregators = [\"AskReddit\", \"WTF\", \"pics\", \"gifs\", \"aww\", \"funny\", \"todayilearned\",\n",
    "                   \"AdviceAnimals\"]\n",
    "relationships = [\"AskMen\", \"AskWomen\", \"relationships\", \"relationship_advice\", \"OkCupid\"]\n",
    "\n",
    "plt.scatter(tsne_df[0].values, tsne_df[1].values, color=\"#dddddd\")\n",
    "plt.scatter(tsne_df[0].values, tsne_df[1].values, color=[\n",
    "    \"green\" if l in games else\n",
    "    \"gold\" if l in trade else\n",
    "    \"purple\" if l in relationships else\n",
    "    \"red\" if l in link_aggregators else\n",
    "    \"blue\" if l in sports else\n",
    "    \"#00000000\"\n",
    "    for l in tsne_df.index])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, video games (e.g., tf2, DotA2; red) tend to group together, along with buy-sell subreddits (e.g., tf2trade, Dota2Trade, yellow), subreddits related to relationships (e.g., AskWomen, relationship_advice; green) and large default-subreddit hubs for sharing random links (e.g., pics, AskReddit; purple).\n",
    "There may be other interesting groupings that you may spot as well -- for instance, a vaguely right-wing MensRights cluster (with Libertarian, guns); a rather intriguing cluster consisting of politics, sex, business, etc.\n",
    "Some topical groups are more diffuse -- for instance, sports-based subreddits (blue). Perhaps this is a limitation of our representation, or that these subreddits actually foster very different interactional dynamics.\n",
    "\n",
    "Another way to delve into these groupings is to look at nearest neighbors of subreddits, in terms of the embedding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = pairwise_distances(subreddit_df.values, metric='cosine')\n",
    "flat_dists = np.ravel(dists)\n",
    "idx1, idx2 = np.unravel_index(np.arange(len(flat_dists)), dists.shape)\n",
    "pairwise_dist_df = pd.DataFrame.from_dict({'p1': subreddit_df.index[idx1],\n",
    "                                           'p2': subreddit_df.index[idx2],\n",
    "                                           'dist': flat_dists},\n",
    "                                           orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nearest_neighbors(pairwise_dist_df, \n",
    "                           test_subreddits=[],\n",
    "                           top_N=10):\n",
    "    for subreddit in test_subreddits:\n",
    "        subset_df = pairwise_dist_df[(pairwise_dist_df.p1 == subreddit)\n",
    "                                 & (pairwise_dist_df.p2 != subreddit)]\n",
    "        print(subreddit)\n",
    "        print(subset_df.sort_values('dist')[['p2', 'dist']].head(top_N))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nearest_neighbors(pairwise_dist_df, ['apple', 'politics', 'leagueoflegends',\n",
    "                                          'AskWomen', 'Music', 'pics',\n",
    "                                          'australia', 'Random_Acts_Of_Amazon',\n",
    "                                          'Bitcoin', 'MensRights'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to interpret each dimension of the embedding -- roughly speaking the threads, features and subreddits with extremal values along one dimension could be seen as characterizing a particular \"type\" of discussion, in terms of the discussion structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dimension(dim, ascending=True, n=5):\n",
    "    top_threads = U_df.sort_values(dim, ascending=ascending).head(n)\n",
    "    display(top_threads)\n",
    "    display(V_df.sort_values(dim, ascending=ascending).head(n))\n",
    "    display(subreddit_df.sort_values(dim, ascending=ascending).head(n))\n",
    "    return top_threads.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the first latent dimension divides the space of Reddit discussions between focused dialogues involving 2 people who repeatedly interact, and \"expansionary\" threads involving multiple people who generally only engage once (as with the corresponding dimension discussed in the paper, this echoes the contrast explored in papers such as Backstrom et. al, 2013). At the subreddit level, we see a divide between subreddits that are selling things (perhaps the dialogues consist of a buyer and a seller) and large default link-sharing subreddits like AskReddit and pics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_threads = display_dimension(0, n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic Diversity in Conversations\n",
    "\n",
    "We will now explore the part of the toolkit dealing with giving characteristics to different actors in a conversation. This will likely most fit in with the datasets we've seen in our previous notebook. It attempts to carry out the methods in the [Finding Your Voice: The Linguistic Development of Mental Health Counselors](http://www.cs.cornell.edu/~cristian/Finding_your_voice__linguistic_development.html).\n",
    "\n",
    "Since we cannot publicly release the dataset of counseling conversations used in that paper, we will use the ChangeMyView subreddit as a test case---as such, this notebook is mostly to demonstrate how the functionality works, rather than to suggest any substantive scientific claims about longitudinal behavior change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: DOWNLOAD CORPUS\n",
    "# UNCOMMENT THESE LINES TO DOWNLOAD CORPUS\n",
    "ROOT_DIR = convokit.download('subreddit-changemyview', data_dir=DATA_DIR)\n",
    "\n",
    "# OPTION 2: READ PREVIOUSLY-DOWNLOADED CORPUS FROM DISK\n",
    "# UNCOMMENT THIS LINE AND REPLACE WITH THE DIRECTORY WHERE THE CORPUS IS LOCATED\n",
    "# ROOT_DIR = '<YOUR DIRECTORY>'\n",
    "\n",
    "corpus = Corpus(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.print_summary_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we will set up a data structure mapping each user to their conversations, and each utterance they contributed in the conversation.\n",
    "To do this we call the ```organize_user_convo_history function```, which annotates each User in a corpus with a dict of conversations --> the user's utterances in that conversation, and the timestamp of their first utterance (i.e., when they \"entered\" the conversation).\n",
    "Note that we can specify what counts as participating in a conversation. Here, we omit posts and focus only on comments (such that a user doesn't count as participating if they only submitted the root post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_BLACKLIST = ['[deleted]', 'DeltaBot','AutoModerator']\n",
    "def utterance_is_valid(utterance):\n",
    "    return (utterance.id != utterance.root) and (utterance.user.name not in USER_BLACKLIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.organize_user_convo_history(utterance_filter=utterance_is_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.get_user('ThatBelligerentSloth').meta['n_convos']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.get_user('ThatBelligerentSloth').meta['start_time']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For each user, we maintain a dictionary in their meta information of conversation ID to a record fo the user's behavior in that conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.get_user('ThatBelligerentSloth').meta['conversations']['2wm22t']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to speed up this demo, we will only take the top 100 most active users.\n",
    "To help with this, the get_attribute_table function call gives us a Pandas dataframe where indices correspond to usernames, and which contains the number of comments each user participated in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activities = corpus.get_attribute_table('user',['n_convos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activities.sort_values('n_convos', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_users = user_activities.sort_values('n_convos', ascending=False).head(100).index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_utts = []\n",
    "for user in top_users:\n",
    "    subset_utts += list(corpus.get_user(user).iter_utterances())\n",
    "subset_corpus = Corpus(utterances=subset_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus.print_summary_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to finish setting things up, we will tokenize the utterances using the TextParser transformer (this is somewhat slow; setting the mode to 'tokenize' means we avoid having to perform expensive dependency-parse computations, which we do not need for the present analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.text_processing import TextProcessor, TextParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TextParser(mode='tokenize', output_field='tokens', verbosity=1000)\n",
    "subset_corpus = tokenizer.transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the tokenized output looks like for one utterance (for a more in-depth explanation, check out the TextParser documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus.get_utterance('cos7k4p').get_info('tokens')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The goal of this analysis is to examine how a user's conversational behavior looks like within a single conversation, and then how it evolves over the conversations they take. To demonstrate what this looks like we'll start with a simple attribute, wordcount. First, we count the words in each utterance using the TextProcessor transformer. Note this computes per utterance statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounter = TextProcessor(input_field='tokens', output_field='wordcount', \n",
    "                           proc_fn=lambda sents: sum(len(sent['toks']) for sent in sents), verbosity=25000)\n",
    "subset_corpus = wordcounter.transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus.get_utterance('cos7k4p').get_info('wordcount')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus.get_utterance('cos8ffz').get_info('wordcount')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we aggregate per-utterance statistics over all the utterances a particular user contributed in a conversation. That is, we will turn wordcount into a user,convo-level attribute.\n",
    "We call the ```UserConvoAttrs``` transformer to do this. Here, agg_fn=np.mean means that the user,convo-le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_wordcount = convokit.user_convo_helpers.user_convo_attrs.UserConvoAttrs('wordcount', agg_fn=np.mean)\n",
    "subset_corpus = uc_wordcount.transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformer annotates each conversation in each User object with a (mean) wordcount:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus.get_user('ThatBelligerentSloth').meta['conversations']['2wm22t']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use this aggregate statistic to analyze how users change behavior over time. The particular question here is whether or not users systematically increase or decrease in wordcount, and in the number of utterances contributed to each conversation.\n",
    "To facilitate further analyses, we'll load all the user,convo information pertaining to the attributes we want into a dataframe. We'll use the get_full_attribute_table function to do this (the particular call tells the function to load a table with wordcount and # of utterances at the user,conversation level, and # of conversations i.e., how active the user was, at the user level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_convo_len_df = subset_corpus.get_full_attribute_table(user_convo_attrs=['wordcount','n_utterances'],\n",
    "                                             user_attrs=['n_convos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_convo_len_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform our longitudinal analyses at the level of life-stages: i.e., contiguous blocks of conversations. Here, we compare between the first two life-stages of 10 conversations: how the user behaves in their first 10, versus their 10th to 20th conversations. We say that users systematically increase (or decrease) in an attribute if for a significant majority of users the value of this attribute at one life-stage increases to the next.\n",
    "To this end, we need to aggregate attributes over a life-stage, e.g., mean wordcount. To perform this aggregation we'll use the get_lifestage_attributes function, specifying lifestages of 10 conversations each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lifestage_attributes(attr_df, attr, lifestage_size, agg_fn=np.mean):\n",
    "    aggs = attr_df.groupby(['user', attr_df.convo_idx // lifestage_size])\\\n",
    "        [attr].agg(agg_fn)\n",
    "    aggs = aggs.reset_index().pivot(index='user', columns='convo_idx',\n",
    "                                   values=attr)\n",
    "    return aggs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on the first 20 conversations (i.e., 2 life-stages). We also ignore all users with less than 20 conversations---so we are not biased by survivorship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = user_convo_len_df[(user_convo_len_df.n_convos__user >= 20)\n",
    "                          & (user_convo_len_df.convo_idx < 20)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_wc_df = get_lifestage_attributes(subset, 'wordcount', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_wc_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_wc_df.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lifestage_comparisons(stage_df):\n",
    "    for i in range(stage_df.columns.max()):\n",
    "        \n",
    "        mask = stage_df[i+1].notnull() & stage_df[i].notnull()\n",
    "        c1 = stage_df[i+1][mask]\n",
    "        c0 = stage_df[i][mask]\n",
    "        \n",
    "        print('stages %d vs %d (%d users)' % (i + 1, i, sum(mask)))\n",
    "        n_more = sum(c1 > c0)\n",
    "        n = sum(c1 != c0)\n",
    "        print('\\tprop more: %.3f, binom_p=%.2f' % (n_more/n, stats.binom_test(n_more,n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_lifestage_comparisons(stage_wc_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_convo_len_df = get_lifestage_attributes(subset, 'n_utterances', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_convo_len_df.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at the means, it looks like there's a slight decrease in wordcount across the population from the first to the second lifestage. To check significance, we can compute that % of users who experience this decrease, and see if it's significant per a binomial test against a null proportion of 50% of users (ie., people randomly increase or decrease)\n",
    "We see that this is (almost) significant ... maybe more data would help!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_lifestage_comparisons(stage_convo_len_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll compute some attributes related to linguistic diversity described in the following paper : http://www.cs.cornell.edu/~cristian/Finding_your_voice__linguistic_development.html\n",
    "In short, for each life-stage, we compare the words used by one user in one conversation to the words they use in their other conversations, or the words that others use. As such, this is a user,convo-level attribute. Given our small sample here (and the fact that CMV and crisis counseling conversations are very different), we're not going for any scientific claims, but use the following function calls to demostrate how the pipeline would work.\n",
    "These attributes are all computed through the UserConvoDiversityWrapper transformer, which computes three attributes:\n",
    "\n",
    "div__self: within-diversity in the paper, comparing language use across a user's own conversations\n",
    "\n",
    "div__other: between-diversity in the paper, comparing language use across different users\n",
    "\n",
    "div__adj: relative diversity: between - within. (intuitively, is the diversity coming from users being different from \n",
    "others, beyond being diverse in their own right?)\n",
    "\n",
    "\n",
    "Under the surface, UserConvoDiversityWrapper calls a more general UserConvoDiversity transformer, which allows for computation of how divergent a conversation is from any arbitrary reference set of conversations, beyond life-stages (see the documentation for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import UserConvoDiversityWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucd = convokit.UserConvoDiversityWrapper(lifestage_size=10, max_exp=20,\n",
    "                sample_size=300, min_n_utterances=1, n_iters=50, cohort_delta=60*60*24*30*2, verbosity=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this takes a while to run, especially with more users involved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus = ucd.transform(subset_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_df = subset_corpus.get_full_attribute_table(['div__self','div__other','div__adj', 'tokens', 'n_utterances'], ['n_convos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that one present limitation of this methodology is that it requires a user's activity in a conversation---and in their other conversations---to be substantive enough. if a user doesn't meet the minimum wordcount per conversation, then the function returns np.nan for that particular user,conversation. Filtering out these null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_df = div_df[div_df.div__self.notnull() | div_df.div__other.notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as with the wordcount example, we can make cross-lifestage comparisons. here we unfortunately see no significant population-wide change in either direction. This might be worth exploring with more users, though note that interpreting this result for CMV versus for counseling conversations where users are randomly assigned might be different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in ['div__self','div__other','div__adj']:\n",
    "    print(attr)\n",
    "    stage_df = get_lifestage_attributes(div_df, attr, 10)\n",
    "    print_lifestage_comparisons(stage_df)\n",
    "    print('\\n\\n===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "\n",
    "This notebook was put toegther from a combination of notebooks and code snippets from the GitHub documentation. We recommend checking out the GitHub and documentation to see what other functionality will be helpful for your research projects.\n",
    "\n",
    "#### Forecasting conversations\n",
    "\n",
    "These two Google Colab notebooks ([notebook 1](https://colab.research.google.com/drive/1SH4iMEHdoH4IovN-b9QOSK4kG4DhAwmb), [notebook 2](https://colab.research.google.com/drive/1GvICZN0VwZQSWw3pJaEVY-EQGoO-L5lH)) walk you through using convokits neural models to see how conversations turn (become more personal, angry, etc).\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "convokit also includes a variety of [datasets](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit#datasets) which would be very useful. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
